{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`HyperParameter Tuning in ScikitLearn through GridSearchCV`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning is a critical step in the machine learning model development process. It involves finding the best combination of hyperparameters for your model to achieve optimal performance. Hyperparameters are parameters that are not learned from the data but are set prior to training, and they can significantly impact the model's performance. It's important because selecting the right hyperparameters can make the difference between a model that performs poorly and one that excels.\n",
    "\n",
    "Here's why hyperparameter tuning is important:\n",
    "\n",
    "1. **Improved Model Performance**: Choosing the right hyperparameters can lead to significant improvements in your model's accuracy, precision, recall, and other performance metrics.\n",
    "\n",
    "2. **Avoiding Overfitting and Underfitting**: Properly tuned hyperparameters help prevent overfitting (model learning noise in the training data) and underfitting (model lacking the capacity to capture the underlying patterns in the data).\n",
    "\n",
    "3. **Generalization**: Optimized hyperparameters result in a model that generalizes well to unseen data, making it more robust and useful for real-world applications.\n",
    "\n",
    "4. **Efficient Resource Utilization**: Hyperparameter tuning can help you make more efficient use of computational resources by reducing the need for excessive model training.\n",
    "\n",
    "Here are the steps to perform hyperparameter tuning in scikit-learn:\n",
    "\n",
    "1. **Define a Range of Hyperparameters to Tune**:\n",
    "\n",
    "   - Determine which hyperparameters you want to tune. These may include learning rate, the number of hidden layers in a neural network, the depth of a decision tree, regularization strength, etc.\n",
    "\n",
    "2. **Choose a Scoring Metric**:\n",
    "\n",
    "   - Select a metric to evaluate the model's performance, such as accuracy, mean squared error, F1-score, etc. This metric will guide the tuning process.\n",
    "\n",
    "3. **Create a Validation Set**:\n",
    "\n",
    "   - Split your data into training, validation, and test sets. The validation set is used to evaluate the model's performance with different hyperparameter combinations.\n",
    "\n",
    "4. **Set Up the Hyperparameter Search**:\n",
    "\n",
    "   - Use techniques like Grid Search or Random Search to search through a range of hyperparameter values. Scikit-learn provides `GridSearchCV` and `RandomizedSearchCV` classes for this purpose.\n",
    "\n",
    "5. **Define the Model**:\n",
    "\n",
    "   - Instantiate your machine learning model with default hyperparameters or initial values.\n",
    "\n",
    "6. **Perform Hyperparameter Tuning**:\n",
    "\n",
    "   - Use the hyperparameter search tool (e.g., `GridSearchCV` or `RandomizedSearchCV`) to fit your model with different combinations of hyperparameters on the training data.\n",
    "\n",
    "   ```python\n",
    "   from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "   # Define hyperparameters and their possible values\n",
    "   param_grid = {'parameter_name': [value1, value2, ...]}\n",
    "\n",
    "   # Create a grid search instance\n",
    "   grid_search = GridSearchCV(estimator, param_grid, scoring='desired_metric', cv=5)\n",
    "\n",
    "   # Fit the grid search to the training data\n",
    "   grid_search.fit(X_train, y_train)\n",
    "   ```\n",
    "\n",
    "7. **Select the Best Hyperparameters**:\n",
    "\n",
    "   - After hyperparameter tuning, you can access the best hyperparameters and the best model from the search.\n",
    "\n",
    "   ```python\n",
    "   best_params = grid_search.best_params_\n",
    "   best_model = grid_search.best_estimator_\n",
    "   ```\n",
    "\n",
    "8. **Evaluate the Model**:\n",
    "\n",
    "   - Evaluate the best model on the validation set or using cross-validation to ensure it performs well.\n",
    "\n",
    "9. **Test on the Test Set**:\n",
    "   - Finally, test the best model on a separate test set to assess its generalization performance.\n",
    "\n",
    "Hyperparameter tuning can be a computationally expensive process, especially when trying a large number of hyperparameter combinations. However, it's crucial for optimizing your model's performance and ensuring it works effectively in real-world applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_theme(context='notebook', style='darkgrid',\n",
    "              palette='dark', font_scale=1.2)\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the Accuracy with random hyper-parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy-score:0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(\n",
    "    n_estimators=1, min_samples_split=2, min_samples_leaf=1)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy-score:{accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`With Those Params our accuracy came out to be about 93%`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now with HyperParameter Tunning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestClassifier(n_estimators=1),\n",
       "             param_grid={'min_samples_leaf': (1, 2),\n",
       "                         'min_samples_split': (2, 3, 5),\n",
       "                         'n_estimators': (5, 10, 2, 20, 14)})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This param_grid will contain all the hyper-parameter and their corresponding values we want it to have\n",
    "# GridSearchCV use all the combinations of\n",
    "# The More the value the more time GridSearchCV will takes as it try out all possible combinations\n",
    "param_grid = {\n",
    "    'n_estimators': (5, 10, 2, 20, 14),\n",
    "    'min_samples_split': (2, 3, 5),\n",
    "    'min_samples_leaf': (1, 2)\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params:{'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 10}\n",
      "Best Estimator:RandomForestClassifier(min_samples_split=5, n_estimators=10)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best Params:{grid_search.best_params_}\")\n",
    "print(f\"Best Estimator:{grid_search.best_estimator_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy-score after GridSearchCV:0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "# Use the best estimator as clf/model\n",
    "clf = grid_search.best_estimator_\n",
    "# clf.fit(X_train,y_train)\n",
    "grid_search_y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, grid_search_y_pred)\n",
    "print(f\"Accuracy-score after GridSearchCV:{accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`After GridSearchCV our Accuracy shoots up to 96%`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note\n",
    "\n",
    "- GridSearchCV can also lead to Overfitting so be carefull about that too\n",
    "- GridSearchCV can be computationally expensive process, especially when trying a large number of hyperparameter combinations.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
